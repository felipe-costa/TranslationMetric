{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jieba\n",
    "import re\n",
    "import codecs\n",
    "import time\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install jieba\n",
    "#!pip install gensim\n",
    "#https://github.com/Lipairui/Text-similarity-centroid-of-the-word-vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `init_sims` (Use fill_norms() instead. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "model_path = 'C:\\\\Temp\\\\wordvectors\\\\GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(model_path,binary=True,unicode_errors='ignore')\n",
    "stopwords_path = 'english-stopwords.txt'\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_doc(stopwords,doc):\n",
    "    '''\n",
    "    Function: preprocess data in Chinese including cleaning, tokenzing...\n",
    "    Input: document string\n",
    "    Output: list of words\n",
    "    '''     \n",
    "    doc = doc.lower()\n",
    "    doc = word_tokenize(doc)\n",
    "    doc = [word for word in doc if word not in set(stopwords)]\n",
    "    doc = [word for word in doc if word.isalpha()]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_vector(model,doc):\n",
    "    '''\n",
    "    Function:\n",
    "        compute the mean of word vectors\n",
    "    Input:\n",
    "        model: gensim word2vec model\n",
    "        doc: list of words\n",
    "    Output:\n",
    "        doc vector \n",
    "    '''\n",
    "    # remove out-of-vocab words\n",
    "    doc = [word for word in doc if word in model.key_to_index]\n",
    "    return np.mean(model[doc],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(model,doc1,doc2):\n",
    "    '''\n",
    "    Function:\n",
    "        calculate cosine similarity of document pair\n",
    "    Input: \n",
    "        model: gensim word2vec model\n",
    "        doc1: list of words of document1\n",
    "        doc2: list of words of document2\n",
    "    Output:\n",
    "        similarity of doc1 and doc2: (float)\n",
    "            value ranges from 0 to 1;\n",
    "            -1 means error\n",
    "    '''\n",
    "    vec1 = np.array(doc_vector(model,doc1)).reshape(1,-1)\n",
    "    vec2 = np.array(doc_vector(model,doc2)).reshape(1,-1)\n",
    "    cos = cosine_similarity(vec1,vec2)[0][0]      \n",
    "    # regularize value of cos to [-1,1]\n",
    "    if cos<-1.0:cos=-1.0\n",
    "    if cos>1.0:cos=1.0      \n",
    "    sim = 1-np.arccos(cos)/np.pi \n",
    "    return sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularize_sim(sims):\n",
    "    '''\n",
    "    Function: replace illegal similarity value -1 with mean value\n",
    "    Input: list of similarity of document pairs\n",
    "    Output: regularized list of similarity \n",
    "    '''\n",
    "    sim_mean = np.mean([sim for sim in sims if sim!=-1])\n",
    "    r_sims = []\n",
    "    errors = 0\n",
    "    for sim in sims:\n",
    "        if sim==-1:\n",
    "            r_sims.append(sim_mean)\n",
    "            errors += 1\n",
    "        else:\n",
    "            r_sims.append(sim)\n",
    "    return r_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_sim(lang,docs1,docs2):\n",
    "    '''\n",
    "    Function:\n",
    "        calculate similarity of document pairs \n",
    "    Input: \n",
    "        lang: text language-Chinese for 'cn'/ English for 'en'\n",
    "        docs1:  document strings list1\n",
    "        docs2: document strings list2\n",
    "    Output:\n",
    "        similarity list of docs1 and docs2 pairs:\n",
    "            value ranges from 0 to 1;\n",
    "            -1 means error\n",
    "    '''\n",
    "    # check if the number of documents matched\n",
    "\n",
    "    assert lang=='cn' or lang=='en', 'Language setting is wrong'\n",
    "    \n",
    "    # preprocess data\n",
    "    stopwords= [w.strip() for w in codecs.open(stopwords_path, 'r',encoding='utf-8').readlines()]\n",
    "    sims = []\n",
    "    for i in range(len(docs1)):        \n",
    "        p1 = tokenize_doc(stopwords,docs1[i])\n",
    "        p2 = tokenize_doc(stopwords,docs2[i])\n",
    "       # calculate similarity\n",
    "        sim = calculate_similarity(model,p1,p2)\n",
    "        sims.append(sim)\n",
    "    # regularize sims\n",
    "    r_sims = regularize_sim(sims)\n",
    "    return r_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7321356712144432, 0.6884806689162883, 0.5777315231179934]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# English text example\n",
    "reference = ['a speaker presents some products',\n",
    "                 'vegetable is being sliced.',\n",
    "                'man sitting using tool at a table in his home.']\n",
    "translation = ['the speaker is introducing the new products on a fair.',\n",
    "                'someone is slicing a tomato with a knife on a cutting board.',\n",
    "                'The president comes to China']\n",
    "\n",
    "doc_sim('en',reference,translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del LogInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline import Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 4 required positional arguments: 'df', 'language', 'model', and 'stopwords'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-79db6d4dde32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbaseline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBaseline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 4 required positional arguments: 'df', 'language', 'model', and 'stopwords'"
     ]
    }
   ],
   "source": [
    "baseline = Baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
